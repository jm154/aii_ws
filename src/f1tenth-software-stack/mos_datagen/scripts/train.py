#!/usr/bin/env python3
import os
import argparse
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import cv2  # OpenCV ë©€í‹°ìŠ¤ë ˆë”© ì¶©ëŒ ë°©ì§€ (í•„ìˆ˜)
from torch.utils.data import DataLoader, ConcatDataset
from torch.optim.lr_scheduler import ReduceLROnPlateau # â­ï¸ ìŠ¤ì¼€ì¤„ëŸ¬ ì¶”ê°€

from model import ClusterFlowNet
from dataset import ClusterDataset

# ---------------- hyperparams ----------------
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# RTX 4070 Ti ìµœì í™” ì„¤ì •
BATCH_SIZE = 64     
NUM_EPOCHS = 100    # ìŠ¤ì¼€ì¤„ëŸ¬ ë™ì‘ì„ ìœ„í•´ ë„‰ë„‰í•˜ê²Œ ì„¤ì •
LR = 1e-4
NUM_WORKERS = 4     
# ---------------------------------------------

# DataLoader worker ì¶©ëŒ ë°©ì§€
cv2.setNumThreads(0)

def train_one_epoch(model, dataloader, optimizer, epoch):
    model.train()
    running_loss = 0.0
    seen_steps = 0

    # ==========================================
    # âš¡ï¸ Hyperparameters
    # ==========================================
    DYNAMIC_WEIGHT = 1.0      # ë™ì  ê°ì²´ ê°€ì¤‘ì¹˜
    COSINE_WEIGHT = 0.5       # ë°©í–¥ Loss ê°€ì¤‘ì¹˜ (MSEì™€ ìŠ¤ì¼€ì¼ ë§ì¶¤)
    GRAD_CLIP_NORM = 2.0      # Gradient Clipping ì„ê³„ê°’
    # ==========================================

    for batch_idx, batch in enumerate(dataloader):
        curr_in = batch[0].to(DEVICE)
        prev_in = batch[1].to(DEVICE)
        ego_vector = batch[2].to(DEVICE)
        raw_ego_vel = batch[3].to(DEVICE)
        target_vel = batch[4].to(DEVICE)
        
        # ë¼ë²¨ ë¡œë“œ (ì—†ìœ¼ë©´ ìë™ ìƒì„±)
        if len(batch) > 5:
            labels = batch[5].to(DEVICE).view(-1)
        else:
            target_speed = torch.norm(target_vel, dim=1)
            labels = (target_speed > 0.5).long()

        optimizer.zero_grad()
        
        # Forward
        output = model(curr_in, prev_in, ego_vector, raw_ego_vel)
        if isinstance(output, tuple):
            pred_vel = output[0]
        else:
            pred_vel = output

        # ìœ íš¨ ë°ì´í„° ë§ˆìŠ¤í‚¹
        mask = ~torch.isnan(target_vel).any(dim=1)
        if mask.sum() == 0: continue
            
        valid_pred = pred_vel[mask]
        valid_target = target_vel[mask]
        valid_labels = labels[mask]

        # -----------------------------------------------------------
        # ğŸ”¥ Hybrid Loss: MSE + Cosine Direction
        # -----------------------------------------------------------
        
        # 1. MSE Loss (ê¸°ë³¸: í¬ê¸° + ë°©í–¥)
        #    reduction='none'ìœ¼ë¡œ ìƒ˜í”Œë³„ ì˜¤ì°¨ ê³„ì‚°
        mse_per_sample = F.mse_loss(valid_pred, valid_target, reduction='none').mean(dim=1)

        # 2. Cosine Similarity Loss (ë°©í–¥ ì§‘ì¤‘)
        # 
        #    Target ì†ë„ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´(ì •ì§€) ë°©í–¥ ì •ì˜ ë¶ˆê°€ -> ë§ˆìŠ¤í‚¹ í•„ìš”
        target_norm = torch.norm(valid_target, dim=1)
        #    ì†ë„ê°€ 0.1 m/s ì´ìƒì¸ ê²½ìš°ë§Œ ë°©í–¥ ì˜¤ì°¨ ê³„ì‚°
        direction_mask = (target_norm > 0.1)
        
        cosine_loss_per_sample = torch.zeros_like(mse_per_sample)
        if direction_mask.sum() > 0:
            # Cosine Simì€ 1(ì¼ì¹˜) ~ -1(ë°˜ëŒ€).
            # Lossë¡œ ì“°ë ¤ë©´: 1 - Cosine (0:ì¼ì¹˜, 2:ë°˜ëŒ€)
            cos_sim = F.cosine_similarity(valid_pred[direction_mask], valid_target[direction_mask], dim=1)
            cosine_loss_per_sample[direction_mask] = 1.0 - cos_sim

        # 3. ê°€ì¤‘ì¹˜ ì ìš© (Total Loss)
        #    Dynamic ê°ì²´ì— ê°€ì¤‘ì¹˜(5ë°°) ì ìš©
        weights = torch.ones_like(mse_per_sample)
        weights[valid_labels == 1] = DYNAMIC_WEIGHT
        
        #    ìµœì¢… ê²°í•©: (MSE + 0.5 * Cosine) * Dynamic_Weight
        total_loss_per_sample = mse_per_sample + (COSINE_WEIGHT * cosine_loss_per_sample)
        loss = (total_loss_per_sample * weights).mean()
        # -----------------------------------------------------------

        loss.backward()
        
        # âš¡ï¸ Gradient Clipping (Loss Spike ë°©ì§€)
        # 
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=GRAD_CLIP_NORM)
        
        optimizer.step()

        running_loss += loss.item()
        seen_steps += 1

        if batch_idx % 100 == 0:
            print(f"[Epoch {epoch}] Batch {batch_idx}: Hybrid Loss={loss.item():.4f}")

    epoch_loss = (running_loss / seen_steps) if seen_steps > 0 else 0.0
    return epoch_loss

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--data-root", type=str, default="../dataset_l")
    parser.add_argument("--epochs", type=int, default=NUM_EPOCHS)
    parser.add_argument("--batch-size", type=int, default=BATCH_SIZE)
    parser.add_argument("--lr", type=float, default=LR)
    parser.add_argument("--num-workers", type=int, default=NUM_WORKERS)
    args = parser.parse_args()

    print(f"Loading data from {args.data_root}...")
    print(f"Settings: Batch={args.batch_size}, Workers={args.num_workers}, Device={DEVICE}")
    print("Optimization: Label-Based Weight x3 + LR Scheduler Active")
    
    # ì—¬ëŸ¬ ì‹œë‚˜ë¦¬ì˜¤(í•˜ìœ„ ë””ë ‰í† ë¦¬) ìë™ ë³‘í•© ë¡œì§
    if os.path.exists(args.data_root):
        subdirs = [os.path.join(args.data_root, d) for d in os.listdir(args.data_root) if os.path.isdir(os.path.join(args.data_root, d))]
    else:
        subdirs = []
    
    datasets = []
    if len(subdirs) > 0:
        print(f"Found {len(subdirs)} scenarios. Merging...")
        for d in subdirs:
            try:
                ds = ClusterDataset(root=d, split="train", num_points=64)
                if len(ds) > 0:
                    datasets.append(ds)
                    print(f"  -> Loaded: {d} ({len(ds)} samples)")
            except Exception as e:
                print(f"  -> Skipping {d}: {e}")
                
        if len(datasets) > 0:
            train_dataset = ConcatDataset(datasets)
            print(f"Total Combined Samples: {len(train_dataset)}")
        else:
            print("  -> No valid datasets found in subdirectories. Trying root directly.")
            train_dataset = ClusterDataset(root=args.data_root, split="train", num_points=64)
    else:
        train_dataset = ClusterDataset(root=args.data_root, split="train", num_points=64)

    # DataLoader ìƒì„±
    train_loader = DataLoader(
        train_dataset, 
        batch_size=args.batch_size, 
        shuffle=True, 
        num_workers=args.num_workers, 
        pin_memory=True,
        drop_last=True 
    )
    
    print(f"Initializing Model on {DEVICE}...")
    model = ClusterFlowNet().to(DEVICE)
    optimizer = optim.Adam(model.parameters(), lr=args.lr)

    # â­ï¸ ìŠ¤ì¼€ì¤„ëŸ¬ ì •ì˜: Lossê°€ 5 epoch ë™ì•ˆ ê°œì„  ì•ˆë˜ë©´ LRì„ ì ˆë°˜(0.5)ìœ¼ë¡œ ì¤„ì„
    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)

    print("Starting Training...")
    for epoch in range(1, args.epochs + 1):
        epoch_loss = train_one_epoch(model, train_loader, optimizer, epoch)
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
        scheduler.step(epoch_loss)
        
        # í˜„ì¬ LR í™•ì¸
        current_lr = optimizer.param_groups[0]['lr']
        print(f"Epoch {epoch} Finished. Avg Loss: {epoch_loss:.6f} | LR: {current_lr:.2e}")

        if epoch % 5 == 0:
            ckpt_path = f"checkpoint_epoch_{epoch}.pth"
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(), # ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœë„ ì €ì¥
            }, ckpt_path)
            print(f"Checkpoint saved: {ckpt_path}")

if __name__ == "__main__":
    main()
